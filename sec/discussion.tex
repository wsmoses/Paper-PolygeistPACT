\section{Discussion}

\subsection{Limitations}
\paragraph{Frontend}
While \tool could technically accept any valid C or C++ thanks to building off Clang, it has the following limitations. Only \icode{struct}s with values of the same type or are used within specific functions (such as \texttt{FILE} within \texttt{fprintf}) are supported due to the lack of a struct-type in high-level MLIR dialects.  All functions that allocate memory must be compiled with \tool and not a C++ compiler to ensure that a \memref is emitted rather than a pointer.

\paragraph{Optimizer}
The limitations of the optimizer are inherited from those of the tools involved.
In particular, the MLIR affine value categorization results in all-or-nothing modeling, degrading any loop to non-affine if it contains even one non-affine access or a negative step. Running \tool's backend on code not generated by \tool's frontend, which reverses loops with negative steps, is limited to loops with positive indices. Finally, MLIR does not yet provide extensive support for non-convex sets (typically expressed as unions). Work is ongoing within MLIR to address such issues.

\paragraph{Experiments}
While our experiments clearly demonstrate the benefits of the techniques implemented in \tool --- statement splitting and late (reduction) parallelization --- non-negligible effects are due to scheduler difference: Pluto in \tool and \icode{isl} in Polly. The version of Polly using Pluto\footnote{\url{http://pluto-compiler.sourceforge.net/\#libpluto}} is not compatible with modern LLVM necessary to leverage MLIR. Connecting \icode{isl} scheduler to \tool may have yielded results closer to Polly, but still not comparable more directly because of the interplay between \scop detection, statement formation and affine scheduling.

\subsection{Opportunities and Future Work}\label{sec:opportunites}
Connecting MLIR to existing polyhedral flows opens numerous avenues for compiler optimization research, connecting polyhedral and conventional SSA-based compiler transformations.
This gives polyhedral schedulers access to important analyses such as aliasing and useful information such as precise data layout and target machine description.
Arguably, this information is already leveraged by Polly, but the representational mismatch between LLVM IR and affine loops makes it difficult to exploit them efficiently.
%This abstraction gap requires complex analyses in the polyhedral optimizer such as scalar dependence removal~\cite{delicm} or array delinearization~\cite{delinearization}.
MLIR exposes similar information at a sufficiently high level to make it usable in affine transformations.

By mixing abstractions in a single module, MLIR provides finer-grain control over the entire transformation process.
An extension of \tool can, e.g., ensure loop vectorization by directly emitting vector instructions instead of relying on pragmas, which are often merely a recommendation for the compiler. The flow can also control lower-level mechanisms like prefetching or emit specialized hardware instructions.
Conversely, polyhedral analyses can guarantee downstream passes that, e.g., address computation never produces out-of-bounds accesses and other information.

% RZ: Would the following paragraph be better? Because you've mentioned how to leverage MLIR for polyhedral, maybe the next thing to describe is the other way around. The examples 
% A polyhedral transformation flow can benefit MLIR as well.
% Based on polyhedral analysis, we can ensure that the loop is vectorized by directly emitting the corresponding vector instructions instead of relying on pragmas, which are often merely a recommendation for the compiler. The flow can also control lower-level mechanisms such as prefetching or emit specialized hardware instructions.
% Similarly, polyhedral analyses can be used to guarantee to downstream passes that address computation never produces out-of-bounds addresses and other useful information.

% RZ: maybe we can move this paragraph after the 1st one, since it also tells how to do better polyhedral analysis by MLIR?
Future work is necessary on controlling statement granularity made possible by \tool. Beyond affecting affine schedules, this technique enables easy rematerialization and local transposition buffers, crucial on GPUs~\cite{gpu_transpose}, as well as software pipelining; all without having to produce C source which is known to be complex~\cite{csmith}. On the other hand, this may have an effect on the compilation time as the number of statements is an important factor in the complexity bound of the dependence analysis and scheduling algorithms.

\subsection{Alternatives}

Instead of allowing polyhedral tools to parse and generate MLIR, one could emit C (or C++) code from MLIR\footnote{\url{https://github.com/marbre/mlir-emitc}} and use C-based polyhedral tools on the C source, but this approach decreases the expressiveness of the flow. Some MLIR constructs, such as parallel reduction loops, can be directly expressed in the polyhedral model, whereas they would require a non-trivial and non-guaranteed raising step in C. Some other constructs, such as prevectorized affine memory operations, cannot be expressed in C at all. \tool enables transparent handling of such constructs in MLIR-to-MLIR flows, but we leave the details of such handling for future work.

The \tool flow can be similarly connected to other polyhedral formats, in particular \icode{isl}. We choose OpenScop for this work because it is supported by a wider variety of tools. \icode{isl} uses schedule trees~\cite{schedule_trees} to represent the initial and transformed program schedule. Schedule trees are sufficiently close to the nested-operation IR model making the conversion straightforward: ``for'' loops correspond to band nodes (one loop per band dimension), ``if'' conditionals correspond to filter nodes, function-level constants can be included into the context node. The tree structure remains the same as that of MLIR regions. The inverse conversion can be obtained using \icode{isl}'s AST generation facility~\cite{grosser2015polyhedral}.